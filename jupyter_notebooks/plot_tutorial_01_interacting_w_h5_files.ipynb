{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "================================================\n",
    "Tutorial 1: Interacting with Pycroscopy H5 files\n",
    "================================================\n",
    "Suhas Somnath\n",
    "\n",
    "11/11/2017\n",
    "\n",
    "This set of tutorials will serve as examples for using and developing end-to-end workflows for pycroscopy.\n",
    "\n",
    "In this example, we will learn how to interact with pycroscopy formatted h5 files.\n",
    "\n",
    "Introduction\n",
    "=========================\n",
    "We highly recommend reading about the pycroscopy data format - available in the docs.\n",
    "\n",
    "Pycroscopy uses a data-centric approach to data analysis and processing meaning that results from all data analysis and\n",
    "processing are written to the same h5 file that contains the recorded measurements. The Hierarchical Data Format (HDF5)\n",
    "allows data to be stored in multiple datasets in a tree-like manner. However, certain rules and considerations have\n",
    "been made in pycroscopy to ensure consistent and easy access to any data. pycroscopy.hdf_utils contains a lot of\n",
    "utility functions that simplify access to data and this tutorial provides an overview of many of the these functions\n",
    "\n",
    "* Other:\n",
    "    * print_tree <-- done\n",
    "* Searching / Lookup:\n",
    "    * find_dataset\n",
    "    * find_results_groups\n",
    "    * get_all_main\n",
    "    * get_auxillary_datasets\n",
    "    * get_group_refs\n",
    "    * get_h5_obj_refs\n",
    "    * get_source_dataset\n",
    "    * check_for_matching_attrs\n",
    "    * check_for_old\n",
    "* Main dataset - Reading:\n",
    "    * check_if_main <-- done\n",
    "    * get_data_descriptor\n",
    "    * reshape_to_n_dims\n",
    "    * reshape_to_2d\n",
    "* Ancillary datasets related:\n",
    "    * get_formatted_labels\n",
    "    * get_dimensionality\n",
    "    * get_sort_order\n",
    "    * get_unit_values\n",
    "* Attributes - Reading\n",
    "    * get_attr\n",
    "    * get_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suhas\\PycharmProjects\\pycroscopy\\pycroscopy\\__init__.py:25: UserWarning: You are using the unity_dev branch, which is aimed at a 1.0 release for pycroscopy. Be advised - this branch changes very significantly and frequently. It is therefore not meant for usage. Use the master or dev branches for regular purposes.\n",
      "  warn('You are using the unity_dev branch, which is aimed at a 1.0 release for pycroscopy. '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Warning package in case something goes wrong\n",
    "from warnings import warn\n",
    "# Package for downloading online files:\n",
    "try:\n",
    "    # This package is not part of anaconda and may need to be installed.\n",
    "    import wget\n",
    "except ImportError:\n",
    "    warn('wget not found.  Will install with pip.')\n",
    "    import pip\n",
    "    pip.main(['install', 'wget'])\n",
    "    import wget\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "if True:\n",
    "    import sys\n",
    "    sys.path.append(os.path.split(os.path.abspath('.'))[0])\n",
    "    import pycroscopy as px\n",
    "else:\n",
    "    try:\n",
    "        import pycroscopy as px\n",
    "    except ImportError:\n",
    "        warn('pycroscopy not found.  Will install with pip.')\n",
    "        import pip\n",
    "        pip.main(['install', 'pycroscopy'])\n",
    "        import pycroscopy as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................................] 5813796 / 5813796Working on:\n",
      "temp.h5\n"
     ]
    }
   ],
   "source": [
    "# Downloading the example file from the pycroscopy Github project\n",
    "url = 'https://raw.githubusercontent.com/pycroscopy/pycroscopy/master/data/BEPS_small.h5'\n",
    "h5_path = 'temp.h5'\n",
    "_ = wget.download(url, h5_path)\n",
    "\n",
    "print('Working on:\\n' + h5_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pycroscopy uses the h5py python package to access the HDF5 files and its contents.\n",
    "Conventionally, the h5py package is used to create, read, write, and modify h5 files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open the file in read-only mode\n",
    "h5_f = h5py.File(h5_path, mode='r')\n",
    "# Here, h5_f is an active handle to the open file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the contents of this h5 data file\n",
    "=========================================\n",
    "\n",
    "The file contents are stored in a tree structure, just like files on a contemporary computer. The file contains\n",
    "datagroups (similar to file folders) and datasets (similar to spreadsheets).\n",
    "There are several datasets in the file and these store:\n",
    "\n",
    "* The actual measurement collected from the experiment\n",
    "* Spatial location on the sample where each measurement was collected\n",
    "* Information to support and explain the spectral data collected at each location\n",
    "* Since pycroscopy stores results from processing and analyses performed on the data in the same file, these\n",
    "  datasets and datagroups are present as well\n",
    "* Any other relevant ancillary information\n",
    "\n",
    "Soon after opening any file, it is often of interest to list the contents of the file. While one can use the open\n",
    "source software HDFViewer developed by the HDF organization, pycroscopy.hdf_utils also has a simply utility to\n",
    "quickly visualize all the datasets and datagroups within the file within python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the H5 file:\n",
      "/\n",
      "├ Measurement_000\n",
      "  ---------------\n",
      "  ├ Channel_000\n",
      "    -----------\n",
      "    ├ Bin_FFT\n",
      "    ├ Bin_Frequencies\n",
      "    ├ Bin_Indices\n",
      "    ├ Bin_Step\n",
      "    ├ Bin_Wfm_Type\n",
      "    ├ Excitation_Waveform\n",
      "    ├ Noise_Floor\n",
      "    ├ Position_Indices\n",
      "    ├ Position_Values\n",
      "    ├ Raw_Data\n",
      "    ├ Raw_Data-SHO_Fit_000\n",
      "      --------------------\n",
      "      ├ Fit\n",
      "      ├ Guess\n",
      "      ├ Spectroscopic_Indices\n",
      "      ├ Spectroscopic_Values\n",
      "    ├ Spatially_Averaged_Plot_Group_000\n",
      "      ---------------------------------\n",
      "      ├ Bin_Frequencies\n",
      "      ├ Mean_Spectrogram\n",
      "      ├ Spectroscopic_Parameter\n",
      "      ├ Step_Averaged_Response\n",
      "    ├ Spatially_Averaged_Plot_Group_001\n",
      "      ---------------------------------\n",
      "      ├ Bin_Frequencies\n",
      "      ├ Mean_Spectrogram\n",
      "      ├ Spectroscopic_Parameter\n",
      "      ├ Step_Averaged_Response\n",
      "    ├ Spectroscopic_Indices\n",
      "    ├ Spectroscopic_Values\n",
      "    ├ UDVS\n",
      "    ├ UDVS_Indices\n"
     ]
    }
   ],
   "source": [
    "print('Contents of the H5 file:')\n",
    "px.hdf_utils.print_tree(h5_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing datasets and datagroups\n",
    "==================================\n",
    "\n",
    "There are numerous ways to access datasets and datagroups in H5 files. Here are two methods using pycroscopy.hdf_utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"UDVS\": shape (256, 7), type \"<f4\">\n",
      "<HDF5 dataset \"UDVS_Indices\": shape (22272,), type \"<u8\">\n"
     ]
    }
   ],
   "source": [
    "# This function returns all datasets that match even a portion of the name\n",
    "udvs_dsets_2 = px.hdf_utils.find_dataset(h5_f, 'UDVS')\n",
    "for item in udvs_dsets_2:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pycroscopy hdf5 files contain three kinds of datasets:\n",
    "\n",
    "* Main datasets that contain data recorded / computed at multiple spatial locations.\n",
    "* Ancillary datasets that support a main dataset\n",
    "* Other datasets\n",
    "\n",
    "For more information, please refer to the documentation on the pycroscopy data format.\n",
    "\n",
    "We can check which datasets within h5_group are Main datasets using a handy hdf_utils function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dset_name in h5_group:\n",
    "    print(px.hdf_utils.checkIfMain(h5_group[dset_name]), ':\\t', dset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data of interest is almost always contained within Main Datasets. Thus, while all three kinds of datasets can\n",
    "be accessed using the methods shown above, we have a function in hdf_utils that allows us to only list the main\n",
    "datasets within the file / group:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_dsets = px.hdf_utils.get_all_main(h5_f)\n",
    "for dset in main_dsets:\n",
    "    print(dset.name, dset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets above show that the file contains three main datasets. Two of these datasets are contained in a folder\n",
    "called Raw_Data-SHO_Fit_000 meaning that they are results of an operation called SHO_Fit performed on the main\n",
    "dataset Raw_Data. The first of the three main datasets is indeed the Raw_Data dataset from which the latter\n",
    "two datasets (Fit and Guess) were derived.\n",
    "\n",
    "Pycroscopy allows the same operation, such as 'SHO_Fit', to be performed on the same dataset (Raw_Data), multiple\n",
    "times. Each time the operation is performed, a new datagroup is created to hold the new results. Often, we may\n",
    "want to perform a few operations such as:\n",
    "\n",
    "* Find the (source / main) dataset from which certain results were derived\n",
    "* Check if a particular operation was performed on a main dataset\n",
    "* Find all datagroups corresponding to a particular operation (e.g. - SHO_Fit) being applied to a main dataset\n",
    "\n",
    "hdf_utils has a few handy functions that simply many of these use cases:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First get the dataset corresponding to Raw_Data\n",
    "h5_raw = h5_f['/Measurement_000/Channel_000/Raw_Data']\n",
    "\n",
    "print('Instances of operation \"{}\" applied to dataset named \"{}\":'.format('SHO_Fit', h5_raw.name))\n",
    "h5_sho_group_list = px.hdf_utils.findH5group(h5_raw, 'SHO_Fit')\n",
    "print(h5_sho_group_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the SHO_Fit operation was performed on Raw_Data only once, which is why findH5group returned only one\n",
    "datagroup - SHO_Fit_000.\n",
    "\n",
    "Often one may want to check if a certain operation was performed on a dataset with the very same parameters to\n",
    "avoid recomputing the results. hdf_utils has a function for this too:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Parameters already used for computing SHO_Fit on Raw_Data in the file:')\n",
    "print(px.hdf_utils.get_attributes(h5_f['/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000']))\n",
    "print('\\nChecking to see if SHO Fits have been computed on the raw dataset:')\n",
    "print('Using pycroscopy')\n",
    "print(px.hdf_utils.check_for_old(h5_raw, 'SHO_Fit',\n",
    "                                 new_parms={'SHO_fit_method': 'pycroscopy BESHO'}))\n",
    "print('Using BEAM')\n",
    "print(px.hdf_utils.check_for_old(h5_raw, 'SHO_Fit',\n",
    "                                 new_parms={'SHO_fit_method': 'BEAM BESHO'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, while findH5group returned any and all groups corresponding to SHO_Fit being applied to Raw_Data,\n",
    "check_for_old only returned the group(s) where the operation was performed using the same parameters.\n",
    "\n",
    "Let's consider the inverse scenario where we are interested in finding the source dataset from which the known\n",
    "result was derived:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5_sho_group = h5_sho_group_list[0]\n",
    "print('Datagroup containing the SHO fits:')\n",
    "print(h5_sho_group)\n",
    "print('\\nDataset on which the SHO Fit was computed:')\n",
    "h5_source_dset = px.hdf_utils.get_source_dataset(h5_sho_group)\n",
    "print(h5_source_dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing Attributes:\n",
    "=====================\n",
    "\n",
    "HDF5 datasets and datagroups can also store metadata such as experimental parameters. These metadata can be text,\n",
    "numbers, small lists of numbers or text etc. These metadata can be very important for understanding the datasets\n",
    "and guide the analysis routines.\n",
    "\n",
    "h5py offers a basic method for accessing attributes attached to datasets and datagroups. However, more complicated\n",
    "operations such as accessing multiple attributes or accessing the original string value of string attributes can\n",
    "be problematic in python 3. pycroscopy.hdf_utils has a few functions that simplifies the process of accessing\n",
    "attributes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Listing all attributes using get_attributes:\n",
    "attr_dict = px.hdf_utils.get_attributes(h5_meas_group, attr_names=None)\n",
    "for att_name in attr_dict:\n",
    "    print('{} : {}'.format(att_name, attr_dict[att_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# accessing specific attributes only:\n",
    "print(px.hdf_utils.get_attributes(h5_meas_group, attr_names=['VS_mode', 'BE_phase_content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the number value of attributes is not a problem using h5py:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# via the standard h5py library:\n",
    "print(h5_meas_group.attrs['VS_amplitude_[V]'])\n",
    "print(h5_meas_group.attrs['VS_amplitude_[V]'] == 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, accessing string valued attributes and using them for comparison is a problem using the standard h5py\n",
    "library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(h5_meas_group.attrs['VS_measure_in_field_loops'])\n",
    "\n",
    "# comparing the (byte)string value of attributes is a problem with python 3:\n",
    "h5_meas_group.attrs['VS_measure_in_field_loops'] == 'in and out-of-field'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the get_attr function in hdf_utils handles such string complications by itself:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str_val = px.hdf_utils.get_attr(h5_meas_group, 'VS_measure_in_field_loops')\n",
    "print(str_val == 'in and out-of-field')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Datasets via PycroDataset\n",
    "==============================\n",
    "\n",
    "For this example, we will be working with a Band Excitation Polarization Switching (BEPS) dataset acquired from\n",
    "advanced atomic force microscopes. In the much simpler Band Excitation (BE) imaging datasets, a single spectra is\n",
    "acquired at each location in a two dimensional grid of spatial locations. Thus, BE imaging datasets have two\n",
    "position dimensions (X, Y) and one spectroscopic dimension (frequency - against which the spectra is recorded).\n",
    "The BEPS dataset used in this example has a spectra for each combination of three other parameters (DC offset,\n",
    "Field, and Cycle). Thus, this dataset has three new spectral dimensions in addition to the spectra itself. Hence,\n",
    "this dataset becomes a 2+4 = 6 dimensional dataset\n",
    "\n",
    "In pycroscopy, all spatial dimensions are collapsed to a single dimension and similarly, all spectroscopic\n",
    "dimensions are also collapsed to a single dimension. Thus, the data is stored as a two-dimensional (N x P)\n",
    "matrix with N spatial locations each with P spectroscopic datapoints.\n",
    "\n",
    "This general and intuitive format allows imaging data from any instrument, measurement scheme, size, or\n",
    "dimensionality to be represented in the same way. Such an instrument independent data format enables a single\n",
    "set of analysis and processing functions to be reused for multiple image formats or modalities.\n",
    "\n",
    "Main datasets can be thought of as substantially more capable and information-packed than standard datasets\n",
    "since they have (or are linked to) all the necessary information to describe a measured dataset. The additional\n",
    "information contained / linked by Main datasets includes:\n",
    "\n",
    "* the recorded physical quantity\n",
    "* units of the data\n",
    "* names of the position and spectroscopic dimensions\n",
    "* dimensionality of the data in its original N dimensional form etc.\n",
    "\n",
    "While it is most certainly possible to access this information via the native h5py functionality, it can become\n",
    "tedious very quickly.  Pycroscopy's PycroDataset class makes such necessary information and any necessary\n",
    "functionality easily accessible.\n",
    "\n",
    "PycroDataset objects are still h5py.Dataset objects underneath, like all datasets accessed above, but add an\n",
    "additional layer of functionality to simplify data operations. Let's compare the information we can get via the\n",
    "standard h5py library with that from PycroDataset to see the additional layer of functionality. The PycroDataset\n",
    "makes the spectral and positional dimensions, sizes immediately apparent among other things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accessing the raw data\n",
    "pycro_main = main_dsets[0]\n",
    "print('Dataset as observed via h5py:')\n",
    "print()\n",
    "print('\\nDataset as seen via a PycroDataset object:')\n",
    "print(pycro_main)\n",
    "# Showing that the PycroDataset is still just a h5py.Dataset object underneath:\n",
    "print()\n",
    "print(isinstance(pycro_main, h5py.Dataset))\n",
    "print(pycro_main == h5_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main datasets are often linked to supporting datasets in addition to the mandatory ancillary datasets.  The main\n",
    "dataset contains attributes which are references to these datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for att_name in pycro_main.attrs:\n",
    "    print(att_name, pycro_main.attrs[att_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets can be accessed easily via a handy hdf_utils function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(px.hdf_utils.getAuxData(pycro_main, auxDataName='Bin_FFT'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional functionality of PycroDataset is enabled through several functions in hdf_utils. Below, we provide\n",
    "several such examples along with comparisons with performing the same operations in a simpler manner using\n",
    "the PycroDataset object:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A function to describe the nature of the contents within a dataset\n",
    "print(px.hdf_utils.get_data_descriptor(h5_raw))\n",
    "\n",
    "# this functionality can be accessed in PycroDatasets via:\n",
    "print(pycro_main.data_descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Ancillary Datasets\n",
    "========================\n",
    "\n",
    "As mentioned earlier, the ancillary datasets contain information about the dimensionality of the original\n",
    "N-dimensional dataset.  Here we see how we can extract the size and corresponding names of each of the spectral\n",
    "and position dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can use the getAuxData function again to get the ancillary datasets linked with the main dataset:\n",
    "# The [0] slicing is to take the one and only position indices and spectroscopic indices linked with the dataset\n",
    "h5_pos_inds = px.hdf_utils.getAuxData(h5_raw, auxDataName='Position_Indices')[0]\n",
    "h5_spec_inds = px.hdf_utils.getAuxData(h5_raw, auxDataName='Spectroscopic_Indices')[0]\n",
    "\n",
    "# Need to state that the array needs to be of the spectral shape.\n",
    "print('Spectroscopic dimensions:')\n",
    "print(px.hdf_utils.get_formatted_labels(h5_spec_inds))\n",
    "print('Size of each dimension:')\n",
    "print(px.hdf_utils.get_dimensionality(h5_spec_inds))\n",
    "print('Position dimensions:')\n",
    "print(px.hdf_utils.get_formatted_labels(h5_pos_inds))\n",
    "print('Size of each dimension:')\n",
    "print(px.hdf_utils.get_dimensionality(h5_pos_inds[()].T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same tasks can very easily be accomplished via the PycroDataset object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# an alternate way to get the spectroscopic indices is simply via:\n",
    "print(pycro_main.h5_spec_inds)\n",
    "\n",
    "# We can get the spectral / position labels and dimensions easily via:\n",
    "print('Spectroscopic dimensions:')\n",
    "print(pycro_main.spec_dim_descriptors)\n",
    "print('Size of each dimension:')\n",
    "print(pycro_main.spec_dim_sizes)\n",
    "print('Position dimensions:')\n",
    "print(pycro_main.pos_dim_descriptors)\n",
    "print('Size of each dimension:')\n",
    "print(pycro_main.pos_dim_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few cases, the spectroscopic / position dimensions are not arranged in descending order of rate of change.\n",
    "In other words, the dimensions in these ancillary matrices are not arranged from fastest-varying to slowest.\n",
    "To account for such discrepancies, hdf_utils has a very handy function that goes through each of the columns or\n",
    "rows in the ancillary indices matrices and finds the order in which these dimensions vary.\n",
    "\n",
    "Below we illustrate an example of sorting the names of the spectroscopic dimensions from fastest to slowest in\n",
    "a BEPS data file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spec_sort_order = px.hdf_utils.get_sort_order(h5_spec_inds)\n",
    "print('Spectroscopic dimensions arranged as is:')\n",
    "unsorted_spec_labels = px.hdf_utils.get_formatted_labels(h5_spec_inds)\n",
    "print(unsorted_spec_labels)\n",
    "sorted_spec_labels = np.array(unsorted_spec_labels)[np.array(spec_sort_order)]\n",
    "print('Spectroscopic dimensions arranged from fastest to slowest')\n",
    "print(sorted_spec_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When visualizing the data it is essential to plot the data against appropriate values on the X, Y, Z axes.\n",
    "Extracting a simple list or array of values to plot against may be challenging especially for multidimensional\n",
    "dataset such as the one under consideration. Fortunately, hdf_utils has a very handy function for this as well:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5_spec_inds = px.hdf_utils.getAuxData(pycro_main, auxDataName='Spectroscopic_Indices')[0]\n",
    "h5_spec_vals = px.hdf_utils.getAuxData(pycro_main, auxDataName='Spectroscopic_Values')[0]\n",
    "dimension_name = 'DC_Offset'\n",
    "dc_dict = px.hdf_utils.get_unit_values(h5_spec_inds, h5_spec_vals, dim_names=dimension_name)\n",
    "print(dc_dict)\n",
    "dc_val = dc_dict[dimension_name]\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.plot(dc_val)\n",
    "axis.set_title(dimension_name)\n",
    "axis.set_xlabel('Points in dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet again, this process is simpler when using the PycroDataset object:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dv_val = pycro_main.get_spec_values(dim_name=dimension_name)\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.plot(dc_val)\n",
    "axis.set_title(dimension_name)\n",
    "axis.set_xlabel('Points in dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping Data\n",
    "==============\n",
    "\n",
    "Pycroscopy stores N dimensional datasets in a flattened 2D form of position x spectral values. It can become\n",
    "challenging to retrieve the data in its original N-dimensional form, especially for multidimensional datasets\n",
    "such as the one we are working on. Fortunately, all the information regarding the dimensionality of the dataset\n",
    "are contained in the spectral and position ancillary datasets. hdf_utils has a very useful function that can\n",
    "help retrieve the N-dimensional form of the data using a simple function call:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndim_form, success, labels = px.hdf_utils.reshape_to_Ndims(h5_raw, get_labels=True)\n",
    "if success:\n",
    "    print('Succeeded in reshaping flattened 2D dataset to N dimensions')\n",
    "    print('Shape of the data in its original 2D form')\n",
    "    print(h5_raw.shape)\n",
    "    print('Shape of the N dimensional form of the dataset:')\n",
    "    print(ndim_form.shape)\n",
    "    print('And these are the dimensions')\n",
    "    print(labels)\n",
    "else:\n",
    "    print('Failed in reshaping the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole process is simplified further when using the PycroDataset object:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndim_form = pycro_main.get_n_dim_form()\n",
    "print('Shape of the N dimensional form of the dataset:')\n",
    "print(ndim_form.shape)\n",
    "print('And these are the dimensions')\n",
    "print(pycro_main.n_dim_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "two_dim_form, success = px.hdf_utils.reshape_from_Ndims(ndim_form,\n",
    "                                                        h5_pos=h5_pos_inds,\n",
    "                                                        h5_spec=h5_spec_inds)\n",
    "if success:\n",
    "    print('Shape of flattened two dimensional form')\n",
    "    print(two_dim_form.shape)\n",
    "else:\n",
    "    print('Failed in flattening the N dimensional dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Close and delete the h5_file\n",
    "h5_f.close()\n",
    "os.remove(h5_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
