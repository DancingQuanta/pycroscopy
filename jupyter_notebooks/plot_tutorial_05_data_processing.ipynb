{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formalizing Data Processing\n",
    "=======================================\n",
    "\n",
    "**Suhas Somnath**\n",
    "\n",
    "9/8/2017\n",
    "\n",
    "**In this example, we will learn how to write a simple yet formal pycroscopy class for processing data.**\n",
    "\n",
    "Introduction\n",
    "------------\n",
    "Most of code written for scientific research is in the form of single-use / one-off scripts due to a few common reasons:\n",
    "* the author feels that it is the fastest mode to accomplishing a research task \n",
    "* the author feels that they are unlikely to perform the same operation again \n",
    "* the author does not anticipate the possibility that others may need to run their code\n",
    "\n",
    "However, more often than not, nearly all researchers have found that one or more of these assumptions fail and a lot of time is spent on fixing bugs and generalizing / formalizing code such that it can be shared or reused. Moreover, we live in an era of open science where the scientific community and an ever-increasing number of scientific journals are moving towards a paradigm where the data and code need to be made available with journal papers. Therefore, in the interest of saving time, energy, and reputation (you do not want to show ugly code / data. Instead you want to be the paragon of clean intellgible data and code), it makes a lot more sense to formalize (parts of) one's data analysis code.  \n",
    "\n",
    "For many researchers, formalizing data processing or analysis may seem like a daunting task due to the complexity of and the number of sub-operations that need to performed. **pycroscopy.Process** greatly simplifies the process of formalizing code by lifting the burden of memory management, CPU management, parallel computing, interruption handling, etc. off the user. The user only needs to address the operation that needs to be performed on each chunk of data along with the data reading and writing.\n",
    "\n",
    "Data processing / analysis using **pycroscopy.Process** typically only involves a few basic tasks:\n",
    "1. Reading data from file\n",
    "2. Computation\n",
    "3. Writing results to disk\n",
    "\n",
    "This example is based on the parallel computing primer where we fit a dataset containing spectra at each location to a\n",
    "function. While that example focused on comparing serial and parallel computing, we will focus on demonstrating the simplicity with which such a data analysis algorithm can be formalized. \n",
    "\n",
    "This example is a simplification of the pycroscopy.analysis.BESHOFitter class. \n",
    "\n",
    "Import necessary packages\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/syz/PycharmProjects/pycroscopy/pycroscopy/__init__.py:25: UserWarning: You are using the unity_dev branch, which is aimed at a 1.0 release for pycroscopy. Be advised - this branch changes very significantly and frequently. Use the master or dev branches for regular purposes.\n",
      "  warn('You are using the unity_dev branch, which is aimed at a 1.0 release for pycroscopy. '\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import, unicode_literals\n",
    "\n",
    "# The package for accessing files in directories, etc.:\n",
    "import os\n",
    "\n",
    "# Warning package in case something goes wrong\n",
    "from warnings import warn\n",
    "\n",
    "# Package for downloading online files:\n",
    "try:\n",
    "    # This package is not part of anaconda and may need to be installed.\n",
    "    import wget\n",
    "except ImportError:\n",
    "    warn('wget not found.  Will install with pip.')\n",
    "    import pip\n",
    "    pip.main(['install', 'wget'])\n",
    "    import wget\n",
    "\n",
    "# The mathematical computation package:\n",
    "import numpy as np\n",
    "from numpy import exp, abs, sqrt, sum, real, imag, arctan2, append\n",
    "\n",
    "# The package used for creating and manipulating HDF5 files:\n",
    "import h5py\n",
    "\n",
    "# Packages for plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Finally import pycroscopy for certain scientific analysis:\n",
    "if True:\n",
    "    import sys\n",
    "    sys.path.append(os.path.split(os.path.abspath('.'))[0])\n",
    "    import pycroscopy as px\n",
    "else:\n",
    "    try:\n",
    "        import pycroscopy as px\n",
    "    except ImportError:\n",
    "        warn('pycroscopy not found.  Will install with pip.')\n",
    "        import pip\n",
    "        pip.main(['install', 'pycroscopy'])\n",
    "        import pycroscopy as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pycroscopy.Process\n",
    "============\n",
    "\n",
    "Every process class consists of the same basic functions:\n",
    "* **__init__** - instantiates a 'Process' object of this class after validating the inputs.\n",
    "    * In this case pycroscopy.Process takes are of most of the checking - whether h5_main is indeed a Main dataset, etc.\n",
    "* **_create_results_datasets** - creates the HDF5 datasets and Group(s) to store the results.\n",
    "* **_map_function** - the operation that will per be performed on each element in the dataset.\n",
    "    * In this case, we will use the wavelet_peaks() function and then add some post-processing of the results\n",
    "* **test** - This function lets the user test the **map_function** on a unit of data (a single pixel / spectra in this case) to see if it returns the desired results. \n",
    "    * This function is generally very simple to implement\n",
    "    * it saves a lot of computational time by allowing the user to spot-check results before computing on the entire dataset\n",
    "* **_write_results_chunk** - writes the computed results back to the file\n",
    "\n",
    "Note that:\n",
    "\n",
    "* Only the code specific to this process needs to be implemented. The generic portions common to most\n",
    "  Processes will be handled by the Process class.\n",
    "* The other functions such as the sho_function, sho_fast_guess function are all specific to this process. These have\n",
    "  been inherited directly from the BE SHO model.\n",
    "* While the class appears to be large, remember that the majority of it deals with the creation of the datasets to store\n",
    "  the results and the actual function that one would have anyway regardless of serial / parallel computation of the\n",
    "  function. The additional code to turn this operation into a Pycroscopy Process is actually rather minimal. As\n",
    "  described earlier, the goal of the Process class is to modularize and compartmentalize the main sections of the code\n",
    "  in order to facilitate faster and more robust implementation of data processing algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PeakFinder(px.Process):\n",
    "\n",
    "    def __init__(self, h5_main, cores=None):\n",
    "        \"\"\"\n",
    "        Validate the inputs and set some parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h5_main - dataset to compute on\n",
    "        cores - Number of CPU cores to use for computation - Optional\n",
    "        \"\"\"\n",
    "        super(PeakFinder, self).__init__(h5_main, cores=cores)\n",
    "        self.process_name = 'Peak_Finding'\n",
    "        \n",
    "    def test(self, pixel_ind):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        spectra = self.h5_main[pixel_ind]\n",
    "        func = self._map_function\n",
    "        return func(spectra)\n",
    "\n",
    "    def _create_results_datasets(self):\n",
    "        \"\"\"\n",
    "        Creates the datasets an Groups necessary to store the results.\n",
    "        There are only TWO operations happening in this function:\n",
    "        1. Creation of HDF5 group to hold results\n",
    "        2. Creation of a HDF5 dataset to hold results\n",
    "        \n",
    "        Please see examples on utilities for writing pycroscopy HDF5 files for more information\n",
    "        \"\"\"\n",
    "        # create a HDF5 group to hold the results\n",
    "        h5_results_group = px.hdf_utils.create_results_group(self.h5_main, self.process_name)\n",
    "        \n",
    "        # Explicitely stating all the inputs to write_main_dataset\n",
    "        # The process will reduce the spectra at each position to a single value\n",
    "        # Therefore, the result is a 2D dataset with the same number of positions as self.h5_main\n",
    "        results_shape = (self.h5_main.shape[0], 1)\n",
    "        results_dset_name = 'Peak_Response'\n",
    "        results_quantity = 'Amplitude'\n",
    "        results_units = 'V'\n",
    "        pos_dims = None # Reusing those linked to self.h5_main\n",
    "        spec_dims = px.write_utils.Dimension('Empty', 'a. u.', 1)\n",
    "        \n",
    "        \n",
    "        # Create an empty results dataset that will hold all the results\n",
    "        self.h5_results = px.hdf_utils.write_main_dataset(h5_results_group, results_shape, results_dset_name, \n",
    "                                                          results_quantity, results_units, pos_dims, spec_dims, \n",
    "                                                          dtype=np.float32,\n",
    "                                                          h5_pos_inds=self.h5_main.h5_pos_inds,\n",
    "                                                          h5_pos_vals=self.h5_main.h5_pos_vals)\n",
    "        # Note that this function automatically creates the ancillary datasets and links them.\n",
    "        \n",
    "        print('Finshed creating datasets')\n",
    "    \n",
    "    def _write_results_chunk(self):\n",
    "        \"\"\"\n",
    "        Write the computed results back to the H5\n",
    "        In this case, there isn't any more additional post-processing required\n",
    "        \"\"\"            \n",
    "        # write the results to the file \n",
    "        self.h5_results[:, 0] = np.array(self._results)\n",
    "        \n",
    "        # Flush the results to ensure that they have indeed been written to the file\n",
    "        self.h5_main.file.flush()\n",
    "\n",
    "        # Now update the start position\n",
    "        self._start_pos = self._end_pos\n",
    "        # this should stop the computation.        \n",
    "\n",
    "    @staticmethod\n",
    "    def _map_function(spectra, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        As in typical scientific problems, the results from wavelet_peaks() need to be\n",
    "        post-processed\n",
    "        \n",
    "        In this case, the wavelet_peaks() function can sometimes return 0 or more than one peak\n",
    "        for spectra that are very noisy\n",
    "        \n",
    "        Knowing that the peak is typically at the center of the spectra,\n",
    "        we return the central index when no peaks were found\n",
    "        Or the index closest to the center when multiple peaks are found\n",
    "        \n",
    "        Finally once we have a single index, we need to index the spectra by that index\n",
    "        in order to get the amplitude at that frequency.\n",
    "        \"\"\"\n",
    "        \n",
    "        peak_inds = px.analysis.guess_methods.GuessMethods.wavelet_peaks(spectra, peak_widths=[20, 60], peak_step=30)\n",
    "        \n",
    "        central_ind = len(spectra) // 2\n",
    "        if len(peak_inds) == 0:\n",
    "            # too few peaks\n",
    "            # set peak to center of spectra\n",
    "            val = central_ind\n",
    "        elif len(peak_inds) > 1:\n",
    "            # too many peaks\n",
    "            # set to peak closest to center of spectra\n",
    "            dist = np.abs(peak_inds - central_ind)\n",
    "            val = peak_inds[np.argmin(dist)]\n",
    "        else:\n",
    "            # normal situation\n",
    "            val = peak_inds[0]\n",
    "        return np.abs(spectra[val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset\n",
    "================\n",
    "\n",
    "For this example, we will be working with a Band Excitation Piezoresponse Force Microscopy (BE-PFM) imaging dataset\n",
    "acquired from advanced atomic force microscopes. In this dataset, a spectra was collected for each position in a two\n",
    "dimensional grid of spatial locations. Thus, this is a three dimensional dataset that has been flattened to a two\n",
    "dimensional matrix in accordance with the pycroscopy data format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the raw data file from Github:\n",
    "h5_path = 'temp.h5'\n",
    "url = 'https://raw.githubusercontent.com/pycroscopy/pycroscopy/master/data/BELine_0004.h5'\n",
    "if os.path.exists(h5_path):\n",
    "    os.remove(h5_path)\n",
    "_ = wget.download(url, h5_path, bar=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read-only mode\n",
    "h5_file = h5py.File(h5_path, mode='r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The main dataset:\n",
      "------------------------------------\n",
      "<HDF5 dataset \"Raw_Data\": shape (16384, 119), type \"<c8\">\n",
      "located at: \n",
      "/Measurement_000/Channel_000/Raw_Data \n",
      "Data contains: \n",
      "[''] (['a']) \n",
      "Data dimensions and original shape: \n",
      "Position Dimensions: \n",
      "X - size: 128 \n",
      "Y - size: 128 \n",
      "Spectroscopic Dimensions: \n",
      "Frequency - size: 119\n"
     ]
    }
   ],
   "source": [
    "h5_meas_grp = h5_file['Measurement_000']\n",
    "\n",
    "# Accessing the dataset of interest:\n",
    "h5_main = px.PycroDataset(h5_meas_grp['Channel_000/Raw_Data'])\n",
    "print('\\nThe main dataset:\\n------------------------------------')\n",
    "print(h5_main)\n",
    "\n",
    "num_rows, num_cols = h5_main.pos_dim_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider calling test() to check results before calling compute() which computes on the entire dataset and writes back to the HDF5 file\n"
     ]
    }
   ],
   "source": [
    "fitter = PeakFinder(h5_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0048981714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitter.test(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed creating datasets\n",
      "You can abort this computation at any time and resume at a later time!\n",
      "\tIf you are operating in a python console, press Ctrl+C or Cmd+C to abort\n",
      "\tIf you are in a Jupyter notebook, click on \"Kernel\">>\"Interrupt\"\n",
      "Finished parallel computation\n",
      "[0.00200135 0.00322685 0.00272113 0.00312234 0.00257732 0.00305148\n",
      " 0.00268378 0.00300558 0.00290046 0.00285854 0.00274262 0.00277178\n",
      " 0.00440468 0.00514442 0.00387373 0.00489817 0.00516523 0.00460631\n",
      " 0.00151064 0.00235155 0.00232911 0.00010282 0.00141946 0.00056969\n",
      " 0.00288982 0.00265272 0.00141847 0.00286329 0.00421067 0.00333027\n",
      " 0.00097382 0.00137351 0.00066657 0.00028706 0.00070315 0.0007369\n",
      " 0.00050666 0.00169736 0.00175795 0.00014987 0.00144386 0.00425164\n",
      " 0.00322629 0.0014094  0.00093455 0.00073421 0.00121491 0.00117978\n",
      " 0.00135916 0.00373153 0.00655445 0.00656324 0.00810891 0.00848985\n",
      " 0.0075443  0.00756923 0.00507225 0.00301273 0.00027978 0.0012216\n",
      " 0.00132498 0.00134131 0.0017612  0.00189815 0.00168737 0.00119111\n",
      " 0.00220136 0.00173145 0.00227724 0.00194873 0.00111256 0.00225473\n",
      " 0.00138607 0.00127275 0.00165657 0.00121411 0.00181285 0.00171642\n",
      " 0.002545   0.00080636 0.00170679 0.00174707 0.00155204 0.00163484\n",
      " 0.00156294 0.00173365 0.00151879 0.00171448 0.00155977 0.0017518\n",
      " 0.00168196 0.00183937 0.00142357 0.00188977 0.00126198 0.0018756\n",
      " 0.0017709  0.00140408 0.00203932 0.00124209]\n",
      "float32 16384\n",
      "Completed computation on chunk. Writing to file.\n"
     ]
    }
   ],
   "source": [
    "h5_results = fitter.compute(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_ind, col_ind = 103, 19\n",
    "pixel_ind = col_ind + row_ind * num_cols\n",
    "spectra = h5_main[pixel_ind]\n",
    "\n",
    "peak_inds = wavelet_peaks(spectra, peak_widths=[20, 60], peak_step=30)\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.scatter(np.arange(len(spectra)), np.abs(spectra), c='black')\n",
    "axis.axvline(peak_inds[0], color='r', linewidth=2)\n",
    "axis.set_ylim([0, 1.1 * np.max(np.abs(spectra))]);\n",
    "axis.set_title('wavelet_peaks found peaks at index: {}'.format(peak_inds), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read-only mode\n",
    "h5_file = h5py.File(h5_path, mode='r+')\n",
    "\n",
    "# Get handles to the the raw data along with other datasets and datagroups that contain necessary parameters\n",
    "h5_meas_grp = h5_file['Measurement_000']\n",
    "num_rows = px.hdf_utils.get_attr(h5_meas_grp, 'grid_num_rows')\n",
    "num_cols = px.hdf_utils.get_attr(h5_meas_grp, 'grid_num_cols')\n",
    "\n",
    "# Getting a reference to the main dataset:\n",
    "h5_main = h5_meas_grp['Channel_000/Raw_Data']\n",
    "\n",
    "# Extracting the X axis - vector of frequencies\n",
    "h5_spec_vals = px.hdf_utils.getAuxData(h5_main, 'Spectroscopic_Values')[-1]\n",
    "freq_vec = np.squeeze(h5_spec_vals.value) * 1E-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ShoGuess class, defined earlier, to calculate the four\n",
    "parameters of the complex gaussian.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = ShoGuess(h5_main, cores=1)\n",
    "h5_results_grp = fitter.compute()\n",
    "h5_guess = h5_results_grp['Guess']\n",
    "\n",
    "row_ind, col_ind = 103, 19\n",
    "pix_ind = col_ind + row_ind * num_cols\n",
    "resp_vec = h5_main[pix_ind]\n",
    "norm_guess_parms = h5_guess[pix_ind]\n",
    "\n",
    "# Converting from compound to real:\n",
    "norm_guess_parms = px.io_utils.compound_to_scalar(norm_guess_parms)\n",
    "print('Functional fit returned:', norm_guess_parms)\n",
    "norm_resp = px.be_sho.SHOfunc(norm_guess_parms, freq_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Amplitude and Phase of the gaussian versus the raw data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, sharex=True, figsize=(5, 10))\n",
    "for axis, func, title in zip(axes.flat, [np.abs, np.angle], ['Amplitude (a.u.)', 'Phase (rad)']):\n",
    "    axis.scatter(freq_vec, func(resp_vec), c='red', label='Measured')\n",
    "    axis.plot(freq_vec, func(norm_resp), 'black', lw=3, label='Guess')\n",
    "    axis.set_title(title, fontsize=16)\n",
    "    axis.legend(fontsize=14)\n",
    "\n",
    "axes[1].set_xlabel('Frequency (kHz)', fontsize=14)\n",
    "axes[0].set_ylim([0, np.max(np.abs(resp_vec)) * 1.1])\n",
    "axes[1].set_ylim([-np.pi, np.pi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delete the temporarily downloaded file**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file.close()\n",
    "os.remove(h5_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
