{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse Sampling\n",
    "==============\n",
    "\n",
    "**Suhas Somnath**\n",
    "\n",
    "4/27/2018\n",
    "\n",
    "Introduction\n",
    "-------------\n",
    "\n",
    "Example scientific problem\n",
    "---------------------------\n",
    "For this example, we will be working with a Band Excitation Piezoresponse Force Microscopy (BE-PFM) imaging dataset\n",
    "acquired from advanced atomic force microscopes. In this dataset, a spectra was collected for each position in a two\n",
    "dimensional grid of spatial locations. Thus, this is a three dimensional dataset that has been flattened to a two\n",
    "dimensional matrix in accordance with the pycroscopy data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/syz/PycharmProjects/pycroscopy/pycroscopy/__init__.py:25: UserWarning: You are using the unity_dev branch, which is aimed at a 1.0 release for pycroscopy. Be advised - this branch changes very significantly and frequently. Use the master or dev branches for regular purposes.\n",
      "  warn('You are using the unity_dev branch, which is aimed at a 1.0 release for pycroscopy. '\n"
     ]
    }
   ],
   "source": [
    "# Ensure python 3 compatibility:\n",
    "from __future__ import division, print_function, absolute_import, unicode_literals\n",
    "\n",
    "# The package for accessing files in directories, etc.:\n",
    "import os\n",
    "\n",
    "# Warning package in case something goes wrong\n",
    "from warnings import warn\n",
    "\n",
    "# Package for downloading online files:\n",
    "try:\n",
    "    # This package is not part of anaconda and may need to be installed.\n",
    "    import wget\n",
    "except ImportError:\n",
    "    warn('wget not found.  Will install with pip.')\n",
    "    import pip\n",
    "    pip.main(['install', 'wget'])\n",
    "    import wget\n",
    "\n",
    "# The mathematical computation package:\n",
    "import numpy as np\n",
    "\n",
    "# The package used for creating and manipulating HDF5 files:\n",
    "import h5py\n",
    "\n",
    "# Packages for plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parallel computation library:\n",
    "try:\n",
    "    import joblib\n",
    "except ImportError:\n",
    "    warn('joblib not found.  Will install with pip.')\n",
    "    import pip\n",
    "    pip.main(['install', 'joblib'])\n",
    "    import joblib\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# A handy python utility that allows us to preconfigure parts of a function\n",
    "from functools import partial\n",
    "\n",
    "# Function for reading the number of CPU cores on this computer:\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Finally import pycroscopy for certain scientific analysis:\n",
    "if True:\n",
    "    import sys\n",
    "    sys.path.append(os.path.split(os.path.abspath('.'))[0])\n",
    "    import pycroscopy as px\n",
    "else:\n",
    "    try:\n",
    "        import pycroscopy as px\n",
    "    except ImportError:\n",
    "        warn('pycroscopy not found.  Will install with pip.')\n",
    "        import pip\n",
    "        pip.main(['install', 'pycroscopy'])\n",
    "        import pycroscopy as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset\n",
    "--------------------\n",
    "In order to demonstrate parallel computing, we will be using a real experimental dataset that is available on the pycroscopy GitHub project. First, lets download this file from Github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the raw data file from Github:\n",
    "h5_path = 'temp.h5'\n",
    "url = 'https://raw.githubusercontent.com/pycroscopy/pycroscopy/master/data/BELine_0004.h5'\n",
    "if os.path.exists(h5_path):\n",
    "    os.remove(h5_path)\n",
    "_ = wget.download(url, h5_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets open this HDF5 file and see its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "├ Measurement_000\n",
      "  ---------------\n",
      "  ├ Channel_000\n",
      "    -----------\n",
      "    ├ Bin_FFT\n",
      "    ├ Bin_Frequencies\n",
      "    ├ Bin_Indices\n",
      "    ├ Bin_Step\n",
      "    ├ Bin_Wfm_Type\n",
      "    ├ Excitation_Waveform\n",
      "    ├ Noise_Floor\n",
      "    ├ Position_Indices\n",
      "    ├ Position_Values\n",
      "    ├ Raw_Data\n",
      "    ├ Spatially_Averaged_Plot_Group_000\n",
      "      ---------------------------------\n",
      "      ├ Bin_Frequencies\n",
      "      ├ Mean_Spectrogram\n",
      "      ├ Spectroscopic_Parameter\n",
      "      ├ Step_Averaged_Response\n",
      "    ├ Spectroscopic_Indices\n",
      "    ├ Spectroscopic_Values\n",
      "    ├ UDVS\n",
      "    ├ UDVS_Indices\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read-only mode\n",
    "h5_file = h5py.File(h5_path, mode='r+')\n",
    "px.hdf_utils.print_tree(h5_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this example is not on the data storage or arrangement but rather on demonstrating parallel computation so lets dive straight into the main dataset that requires fitting of the spectra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The main dataset:\n",
      "------------------------------------\n",
      "<HDF5 dataset \"Raw_Data\": shape (16384, 119), type \"<c8\">\n",
      "located at: \n",
      "/Measurement_000/Channel_000/Raw_Data \n",
      "Data contains: \n",
      "Cantilever Vertical Deflection (V) \n",
      "Data dimensions and original shape: \n",
      "Position Dimensions: \n",
      "X - size: 128 \n",
      "Y - size: 128 \n",
      "Spectroscopic Dimensions: \n",
      "Frequency - size: 119\n"
     ]
    }
   ],
   "source": [
    "# Get handles to the the raw data along with other datasets and datagroups that contain necessary parameters\n",
    "h5_meas_grp = h5_file['Measurement_000']\n",
    "\n",
    "# Accessing the dataset of interest:\n",
    "h5_main = px.PycroDataset(h5_meas_grp['Channel_000/Raw_Data'])\n",
    "px.hdf_utils.write_simple_attrs(h5_main, {'quantity':'Cantilever Vertical Deflection',\n",
    "                                          'units': 'V'})\n",
    "\n",
    "print('\\nThe main dataset:\\n------------------------------------')\n",
    "print(h5_main)\n",
    "\n",
    "num_rows, num_cols = h5_main.pos_dim_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fract = 0.25 \n",
    "chosen_pos = random.sample(range(h5_main.shape[0]), int(fract * h5_main.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(chosen_pos).size == len(chosen_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.tile(np.arange(len(chosen_pos)), (2, 1)).T\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = h5_main.h5_pos_vals[()][chosen_pos]\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = h5_main[()][chosen_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5_sparse_group = px.hdf_utils.create_results_group(h5_main, 'Sparse_Sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5_pos_inds_sparse = h5_sparse_group.create_dataset('Position_Indices', data=indices, \n",
    "                                                    dtype=px.write_utils.INDICES_DTYPE)\n",
    "_ = px.hdf_utils.copy_attributes(h5_main.h5_pos_inds, h5_pos_inds_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5_pos_vals_sparse = h5_sparse_group.create_dataset('Position_Values', data=values, \n",
    "                                                    dtype=px.write_utils.VALUES_DTYPE)\n",
    "_ = px.hdf_utils.copy_attributes(h5_main.h5_pos_vals, h5_pos_vals_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"Sparse_Data\": shape (4096, 119), type \"<c8\">\n",
      "located at: \n",
      "/Measurement_000/Channel_000/Raw_Data-Sparse_Sampling_000/Sparse_Data \n",
      "Data contains: \n",
      "Cantilever Vertical Deflection (V) \n",
      "Data dimensions and original shape: \n",
      "Position Dimensions: \n",
      "X - size: 4096 \n",
      "Y - size: 4096 \n",
      "Spectroscopic Dimensions: \n",
      "Frequency - size: 119\n"
     ]
    }
   ],
   "source": [
    "h5_sparse_main = px.hdf_utils.write_main_dataset(h5_sparse_group, data, 'Sparse_Data', \n",
    "                                                 px.hdf_utils.get_attr(h5_main, 'quantity'), \n",
    "                                                 px.hdf_utils.get_attr(h5_main, 'units'), \n",
    "                                                 None, None,\n",
    "                                                 h5_pos_inds=h5_pos_inds_sparse,\n",
    "                                                 h5_pos_vals=h5_pos_vals_sparse,\n",
    "                                                 h5_spec_inds=h5_main.h5_spec_inds,\n",
    "                                                 h5_spec_vals=h5_main.h5_spec_vals)\n",
    "\n",
    "print(h5_sparse_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Measurement_000\n",
      "├ Channel_000\n",
      "  -----------\n",
      "  ├ Bin_FFT\n",
      "  ├ Bin_Frequencies\n",
      "  ├ Bin_Indices\n",
      "  ├ Bin_Step\n",
      "  ├ Bin_Wfm_Type\n",
      "  ├ Excitation_Waveform\n",
      "  ├ Noise_Floor\n",
      "  ├ Position_Indices\n",
      "  ├ Position_Values\n",
      "  ├ Raw_Data\n",
      "  ├ Raw_Data-Sparse_Sampling_000\n",
      "    ----------------------------\n",
      "    ├ Position_Indices\n",
      "    ├ Position_Values\n",
      "    ├ Sparse_Data\n",
      "  ├ Spatially_Averaged_Plot_Group_000\n",
      "    ---------------------------------\n",
      "    ├ Bin_Frequencies\n",
      "    ├ Mean_Spectrogram\n",
      "    ├ Spectroscopic_Parameter\n",
      "    ├ Step_Averaged_Response\n",
      "  ├ Spectroscopic_Indices\n",
      "  ├ Spectroscopic_Values\n",
      "  ├ UDVS\n",
      "  ├ UDVS_Indices\n"
     ]
    }
   ],
   "source": [
    "px.hdf_utils.print_tree(h5_meas_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5_file.close()\n",
    "os.remove(h5_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
