{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Scientific Workflows in Pycroscopy - Part 1: Translation & Data Format\n",
    "\n",
    "#### Suhas Somnath\n",
    "8/8/2017\n",
    "\n",
    "This set of notebooks will serve as examples for developing and end-to-end workflows for and using pycroscopy. \n",
    "\n",
    "In this example, we describe the pycroscopy data format and transform a __Scanning Tunnelling Spectroscopy (STS)__ raw data file, as obtained from an Omicron STM, to the pycroscopy data format. \n",
    "\n",
    "My hope is that this notebook will serve as a comprehensive example for:\n",
    "\n",
    "1. __Translation__\n",
    "    1. Learning how to read the raw data files obtained from certain microscopes\n",
    "    2. Shaping and structuring the data in a way that is compatible with pycroscopy\n",
    "    3. Writing this data to .h5 files that are used in pycroscopy\n",
    "    \n",
    "2. __Data Access__\n",
    "    1. Loading, reading, writing, and manipulating HDF5 / H5 files.\n",
    "    \n",
    "3. __Data Analysis__\n",
    "    1. Using data analysis routines already present in pycroscopy\n",
    "    \n",
    "4. __Developing Custom Data Processing__\n",
    "    1. Performing some custom data analysis not available in pycroscopy\n",
    "    2. Writing results of this analysis back to the file\n",
    "    \n",
    "5. __Visualization__\n",
    "    1. Visualizing results of analyses and processing using pycroscopy functions\n",
    "    2. Developing simple interactive visualizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why should you care?\n",
    "\n",
    "The quest for understanding more about samples has necessitated the development of a multitude of microscopes, each capable of numerous measurement modalities. \n",
    "\n",
    "Typically, each commercial microscope generates data files formatted in proprietary data formats by the instrument manufacturer. The proprietary natures of these data formats impede scientific progress in the following ways:\n",
    "1. By making it challenging for researchers to extract data from these files \n",
    "2. Impeding the correlation of data acquired from different instruments.\n",
    "3. Inability to store results back into the same file\n",
    "4. Accomodating files from few kilobytes to several gigabytes of data\n",
    "5. Requiring different versions of analysis routines for each format\n",
    "\n",
    "Future concerns:\n",
    "1. Several fields are moving towards the open science paradigm which will require journals and researchers to support journal papers with data and analysis software \n",
    "2. US Federal agencies that support scientific research require curation of datasets in a clear and organized manner\n",
    "\n",
    "To solve these and many more problems, we have developed an __instrument agnostic data format__ that can be used to represent data from any instrument, size, dimensionality, or complexity.\n",
    "\n",
    "## Pycroscopy data format\n",
    "\n",
    "Regardless of origin, modality or complexity, imaging data have one thing in common:\n",
    "* __The same measurement is performed at multiple spatial locations__\n",
    "\n",
    "The data format in pycroscopy is based on this one simple ground truth. The data always has some spatial dimensions (X, Y, Z) and some spectroscopic dimensions (time, frequency, intensity, wavelength, temperature, cycle, voltage, etc.). Pycroscopy, the spatial dimensions are collapsed onto a single dimension and the spectroscopic dimensions are flattened to the other dimensions. Thus, all data are stored as two dimensional grids. Here are some examples of how some familar data can be represented using this paradigm:\n",
    "* __Grayscale photographs__: A single value (intensity) in is recorded at each pixel in a two dimensional grid. Thus, there are are two spatial dimensions - X, Y and one spectroscopic dimension - \"Intensity\". The data can be represented as a N x 1 matrix where N is the product of the number of rows and columns of pixels. The second axis has size of 1 since we only record one value (intensity) at each location. __The positions will be arranged as row0-col0, row0-col1.... row0-colN, row1-col0....__\n",
    "    * In the case of a color image, the data would be of shape N x 3. Where the red, green, blue intensity values would be stored separately. \n",
    "* A __single Raman spectra__: In this case, the measurement is recorded at a single location. At this position, data is recorded as a function of a single (spectroscopic) variable such as wavelength. Thus this data is represented as a 1 x P matrix, where P is the number of points in the spectra\n",
    "* __Scanning Tunelling Spectroscopy or IV spectroscopy__: The current (A 1D array of size P) is recorded as a function of voltage at each position in a two dimensional grid of points (two spatial dimensions). Thus the data would be represente as a N x P matrix, where N is the product of the number of rows and columns in the grid and P is the number of spectroscopic points recorded. \n",
    "    * If the same voltage sweep were performed twice at each location, the data would be represented as N x 2 P. The data is still saved as a long (2*P) 1D array at each location. The number of spectroscopic dimensions would change from just ['Voltage'] to ['Voltage', 'Cycle'] where the second spectroscopic dimension would account for repetitions of this bias sweep.\n",
    "        * __The spectroscopic data would be stored as it would be recorded as volt_0-cycle_0, volt_1-cycle_0..... volt_P-1-cycle_0, volt_0-cycle_1.....volt_P-1-cycle-1. Just like the positions__\n",
    "    * Now, if the bias was swept thrice from -1 to +1V and then thrice again from -2 to 2V, the data bacomes N x 2 * 3 P. The data now has two position dimensions (X, Y) and three spectrosocpic dimensions ['Voltage', 'Cycle', 'Step']. The data is still saved as a (P * 2 * 3) 1D array at each location. \n",
    "    \n",
    "#### Making sense of such flattned datasets:\n",
    "Each main dataset is always accompanied by four ancillary datasets: \n",
    "* the position value and index of each spatial location (row)\n",
    "* the spectroscopic value and index of any column in the dataset\n",
    "In addition to serving as a legend or the key, these ancillary datasets are necessary for explaining:\n",
    "* the original dimensionality of the dataset\n",
    "* how to reshape the data back to its N dimensional form\n",
    "\n",
    "From the __IV Spectorscopy__ example with [X, Y] x [Voltage, Cycle, Step]:\n",
    "* The position datasets would be of shape N x 2 - N total position, two spatial dimensions. \n",
    "    * The position indices datasets may start like: \n",
    "    \n",
    "| 0 | 0 |\n",
    "| 0 | 1 |\n",
    "| a | t |\n",
    "\n",
    "\n",
    "        * 0, 0\n",
    "        * 0, 1\n",
    "        * ....\n",
    "        * 0, N/2\n",
    "        * 1, 0 ....\n",
    "        would be structured exactly\n",
    "\n",
    "#### Channels\n",
    "The pycroscopy data format also allows multiple channels of information to be recorded as separate datasets in the same file. For example, one channel could be a spectra (1D array) collected at each location on a 2D grid while another could be the temperature (single value) recorded by another sensor at the same spatial positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting up the notebook\n",
    "\n",
    "There are a few things that need to be done before any code is written \n",
    "1. If the notebook is intended to work with both python 2 and 3, import from __future__ before importing any other packages\n",
    "2. Next, import packages necessary for use later. \n",
    "3. Set up the plotting backend for matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure python 3 compatibility:\n",
    "from __future__ import division, print_function, absolute_import, unicode_literals\n",
    "\n",
    "# The package for accessing files in directories, etc.:\n",
    "from os import path\n",
    "\n",
    "# The mathematical computation package:\n",
    "import numpy as np\n",
    "\n",
    "# The package used for creating and manipulating HDF5 files:\n",
    "import h5py\n",
    "\n",
    "# Packages for plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Packages for signal filtering and data analysis:\n",
    "from scipy.signal import medfilt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Finally import pycroscopy for certain scientific analysis:\n",
    "import pycroscopy as px\n",
    "from pycroscopy.io.translators.omicron_asc import AscTranslator\n",
    "\n",
    "# set up notebook to show plots within the notebook\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data from raw data files\n",
    "\n",
    "Before any data analysis, we need to access data stored in the files generated by the microscope. Often, the data and parameters in these files are __not__ straightforward to access. In certain cases, additional packages are necessary to access the data while in many other cases, it is possible to extract the necessary information from built-in __numpy__ or similar python packages included with __anaconda__.\n",
    "\n",
    "Pycroscopy aims to make data access, storage, curation, etc. simply by storing the data along with all relevant parameters in a single __.hdf5__ or __.h5__ file. Among the numerous benefits of __HDF5__ files are that these files:\n",
    "* are readily compatible with high-performance computing facilities\n",
    "* scale very efficiently from few kilobytes to several terabytes\n",
    "* can be read and modified using any language including Python, Matlab, C/C++, Java, Fortran, Igor Pro, etc.\n",
    "\n",
    "The process of copying data from the original format to __pycroscopy compatible hdf5__ files is called __Translation__ and the classes available in pycroscopy that perform these operation are called __Translators__\n",
    "\n",
    "__The goal in this section is to trandslate the .asc file obtained from an Omicron microscope into a pycroscopy compatible .h5 file. __\n",
    "While there is an __AscTranslator__ avialable in pycroscopy that can translate these files in just a __single__ line, we will intentionally assume that no such translator is avialable. Using a handful of useful functions in pycroscopy, we will translate the files from the source __.asc__ format to the pycroscopy compatible __.h5__ in just a few lines. The code developed below is essentially the __AscTranslator__. The same methodology can be used to translate other data formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% Load file\n",
    "raw_file_path = px.io.uiGetFile(filter='Omicron STS Files (*.asc)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the instrument generated data file\n",
    "\n",
    "Inherently, one may not know how to read these __.asc__ files. One option is to try and read the file as a text file one line at a time. \n",
    "\n",
    "It turns out that these .asc files are effectively the standard __ASCII__ text files. \n",
    "\n",
    "Here is how we tested to see if the __asc__ files could be interpreted as text files. Below, we read just thefirst 10 lines in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# File Format = ASCII\n",
      "\n",
      "# Created by SPIP 4.6.5.0 2016-09-22 13:32\n",
      "\n",
      "# Original file: C:\\Users\\Administrator\\AppData\\Roaming\\Omicron NanoTechnology\\MATRIX\\default\\Results\\16-Sep-2016\\I(V) TraceUp Tue Sep 20 09.17.08 2016 [14-1]  STM_Spectroscopy STM\n",
      "\n",
      "# x-pixels = 100\n",
      "\n",
      "# y-pixels = 100\n",
      "\n",
      "# x-length = 29.7595\n",
      "\n",
      "# y-length = 29.7595\n",
      "\n",
      "# x-offset = -967.807\n",
      "\n",
      "# y-offset = -781.441\n",
      "\n",
      "# z-points = 500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(raw_file_path, 'r') as file_handle:\n",
    "    for lin_ind in range(10):\n",
    "        print(file_handle.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that these files are simple text files, we can manually go through the file to find out which lines are important, at what lines the data starts etc. \n",
    "\n",
    "Manual investigation of such .asc files revealed that these files are always formatted in the same way. Also, they contain parameters in the first 403 lines and then contain data which is arranged as one pixel per row.\n",
    "\n",
    "STS experiments result in 3 dimensional datasets (X, Y, current). In other words, a 1D array of current data (as a function of excitation bias) is sampled at every location on a two dimensional grid of points on the sample.\n",
    "\n",
    "By knowing where the parameters are located and how the data is structured, it is possible to extract the necessary information from these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Read the entire file to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting the raw data into memory\n",
    "file_handle = open(raw_file_path, 'r')\n",
    "string_lines = file_handle.readlines()\n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 Read the parameters\n",
    "\n",
    "Present in the first few lines of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-section :\t 491\n",
      "x-pixels :\t 100\n",
      "z-unit :\t nV\n",
      "x-length :\t 29.7595\n",
      "z-offset :\t 1116.49\n",
      "x-offset :\t -967.807\n",
      "z-points :\t 500\n",
      "voidpixels :\t 0\n",
      "y-offset :\t -781.441\n",
      "z-range :\t 2000000000\n",
      "value-unit :\t nA\n",
      "y-length :\t 29.7595\n",
      "y-pixels :\t 100\n",
      "scanspeed :\t 59519000000\n"
     ]
    }
   ],
   "source": [
    "# Reading parameters stored in the first few rows of the file\n",
    "parm_dict = dict()\n",
    "for line in string_lines[3:17]:\n",
    "    line = line.replace('# ', '')\n",
    "    line = line.replace('\\n', '')\n",
    "    temp = line.split('=')\n",
    "    test = temp[1].strip()\n",
    "    try:\n",
    "        test = float(test)\n",
    "        # convert those values that should be integers:\n",
    "        if test % 1 == 0:\n",
    "            test = int(test)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    parm_dict[temp[0].strip()] = test\n",
    "\n",
    "# Print out the parameters extracted\n",
    "for key in parm_dict.keys():\n",
    "    print(key, ':\\t', parm_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1 Prepare to read the data\n",
    "\n",
    "Before we read the data, we need to make an empty array to store all this data. In order to do this, we need to read the dictionary of parameters we made in step 2 and extract necessary quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = int(parm_dict['y-pixels'])\n",
    "num_cols = int(parm_dict['x-pixels'])\n",
    "num_pos = num_rows * num_cols\n",
    "spectra_length = int(parm_dict['z-points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2 Read the data\n",
    "\n",
    "Data is present after the first 403 lines of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_headers = len(string_lines) - num_pos\n",
    "num_headers = 403\n",
    "\n",
    "# Extract the STS data from subsequent lines\n",
    "raw_data_2d = np.zeros(shape=(num_pos, spectra_length), dtype=np.float32)\n",
    "for line_ind in range(num_pos):\n",
    "    this_line = string_lines[num_headers + line_ind]\n",
    "    string_spectrum = this_line.split('\\t')[:-1]  # omitting the new line\n",
    "    raw_data_2d[line_ind] = np.array(string_spectrum, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.a Preparing the parameters to pass onto the NumpyTranslator\n",
    "\n",
    "The NumpyTranslator simplifies the ceation of pycroscopy compatible datasets. It handles the file creation, dataset creation and writing, creation of ancillary datasets, datagroup creation, writing parameters, linking ancillary datasets to the main dataset etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_v = 1 # This is the one parameter we are not sure about\n",
    "\n",
    "folder_path, file_name = path.split(raw_file_path)\n",
    "file_name = file_name[:-4] + '_'\n",
    "\n",
    "# Generate the x / voltage / spectroscopic axis:\n",
    "volt_vec = np.linspace(-1 * max_v, 1 * max_v, spectra_length)\n",
    "\n",
    "h5_path = path.join(folder_path, file_name + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4b. Calling the NumpyTranslator to do all the heavy lifting\n",
    "\n",
    "With a single call to the NumpyTranslator, we complete the translation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tran = px.io.NumpyTranslator()\n",
    "h5_path = tran.translate(h5_path, raw_data_2d, num_rows, num_cols, \n",
    "                         qty_name='Current', data_unit='nA', spec_name='Bias', \n",
    "                         spec_unit='V', spec_val=volt_vec, scan_height=100, \n",
    "                         scan_width=200, spatial_unit='nm', data_type='STS', \n",
    "                         translator_name='ASC', parms_dict=parm_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on pycroscopy translation\n",
    "* Steps 1-3 would be performed anyway in order to begin data analysis\n",
    "* The actual pycroscopy translation step are reduced to just 3-4 lines in step 4.\n",
    "* While this approach is feasible and encouraged for simple and small data, it may be necessary to use lower level calls to write efficient translators\n",
    "\n",
    "## Next example  - Reading and Acessing Data\n",
    "* Please see the next notebook in the example series to learn more about reading and accessing data. \n",
    "* We have shown briefly what the file looks like after it is written below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "Measurement_000\n",
      "Measurement_000/Channel_000\n",
      "Measurement_000/Channel_000/Position_Indices\n",
      "Measurement_000/Channel_000/Position_Values\n",
      "Measurement_000/Channel_000/Raw_Data\n",
      "Measurement_000/Channel_000/Spectroscopic_Indices\n",
      "Measurement_000/Channel_000/Spectroscopic_Values\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(h5_path, mode='r') as h5_file:\n",
    "    px.hdf_utils.print_tree(h5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
