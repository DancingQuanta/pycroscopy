{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/syz/PycharmProjects/pycroscopy/pycroscopy/__init__.py:25: UserWarning: You are using the unity_dev branch, which is aimed at a 1.0 release for pycroscopy. Be advised - this branch changes very significantly and frequently. Use the master or dev branches for regular purposes.\n",
      "  warn('You are using the unity_dev branch, which is aimed at a 1.0 release for pycroscopy. '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "sys.path.append(os.path.split(os.path.abspath('.'))[0])\n",
    "import pycroscopy as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "=========\n",
    "\n",
    "The general nature of pycroscopy facilitates the representation of any kind of measurement data.\n",
    " This includes:\n",
    " 1. Conventional data represented using floating point numbers such as 1.2345\n",
    " 2. Integer data (with or without sign) such as 1, 2, 3, 4\n",
    " 3. Complex-valued data such as 1.23 + 4.5i\n",
    " 4. Multi-valued or compound valued data cells such as ('Frequency': 301.2, 'Amplitude':1.553E-3, 'Phase': 2.14)\n",
    "    where a single value or measurement is represented by multiple elements, each with their own names, and data types\n",
    "\n",
    "While HDF5 datasets are capable of storing all of these kinds of data, many conventional data analysis techniques\n",
    " such as decomposition, clustering, etc. are either unable to handle complicated data types such as complex-valued\n",
    " datasets and compound valued datasets, or the results from these techniques do not produce physically meaningful\n",
    " results. For example, most singular value decomposition algorthms are capable of processing complex-valued datasets.\n",
    " However, while the eigenvectors can have complex values, the resultant complex-valued abundance maps are meaningless. These algorithms would not even work if the original data was compound valued!\n",
    " To avoid such problems, we need functions that transform the data to and from the necessary type (integer, real-value etc.)\n",
    "\n",
    "The pycrocsopy.dtype_utils module facilitates comparisons, validations, and most importantly, transformations of one data-type to another. We will be going over the many useful functions in this module and explaining how, when and why one would use them.\n",
    "\n",
    "Utilities for validating data types\n",
    "===================\n",
    "pycroscopy.dtype_utils contains some handy functions that make it easy to write robust and safe code by simplifying common data type checking and validation. \n",
    "\n",
    "contains_integers()\n",
    "-------------------\n",
    "The contains_integers() function checks to make sure that each item in a list is indeed an integer. Additionally, it can be configured to ensure that all the values are above a minimum value. This is particularly useful when building indices matrices based on the size of dimensions - specified as a list of integers for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, -3, 4] : contains integers? : True\n",
      "[1, 4.5, 2.2, -1] : contains integers? : False\n",
      "[1, 5, 8, 3] : contains integers >= 2 ? : False\n"
     ]
    }
   ],
   "source": [
    "item = [1, 2, -3, 4]\n",
    "print('{} : contains integers? : {}'.format(item, px.dtype_utils.contains_integers(item)))\n",
    "item = [1, 4.5, 2.2, -1]\n",
    "print('{} : contains integers? : {}'.format(item, px.dtype_utils.contains_integers(item)))\n",
    "\n",
    "item = [1, 5, 8, 3]\n",
    "min_val = 2\n",
    "print('{} : contains integers >= {} ? : {}'.format(item, min_val, \n",
    "                                                px.dtype_utils.contains_integers(item, min_val=min_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate_dtype()\n",
    "---------------\n",
    "The validate_dtype() function ensure that a provided object is indeed a valid h5py or numpy data type. When writing a main dataset along with all ancillary datasets, pycroscopy meticulously ensures that all intputs are valid before writing data to the file. This comes in very handy when we want to follow the 'measure twice, cut once' ethos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is <class 'numpy.float16'> a valid dtype? : True\n",
      "Is <class 'numpy.complex64'> a valid dtype? : True\n",
      "Is <class 'numpy.uint8'> a valid dtype? : True\n",
      "Is <class 'numpy.int16'> a valid dtype? : True\n"
     ]
    }
   ],
   "source": [
    "for item in [np.float16, np.complex64, np.uint8, np.int16]:\n",
    "    print('Is {} a valid dtype? : {}'.format(item, px.dtype_utils.validate_dtype(item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is especially useful on compound or structured data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is [('r', '<f4'), ('g', '<u2'), ('b', '<f8')] a valid dtype? : True\n"
     ]
    }
   ],
   "source": [
    "struct_dtype = np.dtype({'names': ['r', 'g', 'b'],\n",
    "                        'formats': [np.float32, np.uint16, np.float64]})\n",
    "print('Is {} a valid dtype? : {}'.format(struct_dtype, px.dtype_utils.validate_dtype(struct_dtype)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_compound_sub_dtypes()\n",
    "--------------------------\n",
    "One common hassle when dealing with compund / structured array dtypes is that it can be a little challenging to quickly get the individual datatypes of each field in such a data type. The get_compound_sub_dtypes() makes this a lot easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g : uint16\n",
      "r : float32\n",
      "b : float64\n"
     ]
    }
   ],
   "source": [
    "sub_dtypes = px.dtype_utils.get_compound_sub_dtypes(struct_dtype)\n",
    "for key, val in sub_dtypes.items():\n",
    "    print('{} : {}'.format(key, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_complex_dtype()\n",
    "------------------\n",
    "Quite often, we need to treat complex datasets different from compound datasets which themselves need to be treated different from real valued datasets. is_complex_dtype() makes it easier to check if a numpy or HDF5 dataset has a complex data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is <class 'numpy.float32'> a complex dtype?: False\n",
      "Is <class 'numpy.float16'> a complex dtype?: False\n",
      "Is <class 'numpy.uint8'> a complex dtype?: False\n",
      "Is <class 'numpy.int16'> a complex dtype?: False\n",
      "Is [('r', '<f4'), ('g', '<u2'), ('b', '<f8')] a complex dtype?: False\n",
      "Is <class 'bool'> a complex dtype?: False\n",
      "Is <class 'complex'> a complex dtype?: True\n",
      "Is <class 'numpy.complex64'> a complex dtype?: True\n",
      "Is <class 'numpy.complex128'> a complex dtype?: True\n",
      "Is <class 'numpy.complex256'> a complex dtype?: True\n"
     ]
    }
   ],
   "source": [
    "for dtype in [np.float32, np.float16, np.uint8, np.int16, struct_dtype, bool]:\n",
    "    print('Is {} a complex dtype?: {}'.format(dtype, (px.dtype_utils.is_complex_dtype(dtype))))\n",
    "\n",
    "for dtype in [np.complex, np.complex64, np.complex128, np.complex256]:\n",
    "    print('Is {} a complex dtype?: {}'.format(dtype, (px.dtype_utils.is_complex_dtype(dtype))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation\n",
    "===========\n",
    "Perhaps the biggest benefit of dtype_utils is the ability to flatten complex, compound datasets to real-valued datasets and vice versa. As mentioned in the introduction, this is particularly important when attempting to use machine learning algorithms on complex or compound-valued datasets. In order to enable such pipelines, we need functions to transform:\n",
    "* complex / compound valued datasets to real-valued datasets\n",
    "* real-valued datasets back to complex / compound valued datasets \n",
    "\n",
    "flatten_complex_to_real()\n",
    "------------------------\n",
    "As the name suggests, this function stacks the imaginary values of a N-dimensional numpy / HDF5 dataset below its real-values. Thus, applying this function to a complex valued dataset of size (a, b, c) would result in a real-valued dataset of shape (a, b, 2 * c):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex value: [ 0.-2.j -1.+1.j -2.-3.j] has shape: (3,)\n",
      "Stacked real value: [ 0. -1. -2. -2.  1. -3.] has shape: (6,)\n"
     ]
    }
   ],
   "source": [
    "length = 3\n",
    "complex_array = np.random.randint(-5, high=5, size=(length)) + 1j* np.random.randint(-5, high=5, size=(length))\n",
    "stacked_real_array = px.dtype_utils.flatten_complex_to_real(complex_array)\n",
    "print('Complex value: {} has shape: {}'.format(complex_array, complex_array.shape))\n",
    "print('Stacked real value: {} has shape: '\n",
    "      '{}'.format(stacked_real_array, stacked_real_array.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten_compound_to_real()\n",
    "-------------------------\n",
    "This function flattens a compound-valued dataset of shape (a, b, c) into a real-valued dataset of shape (a, b, k * c) where k is the number of fields within the structured array / compound dtype. Here we will demonstrate this on a 1D array of 5 elements each containing 'r', 'g', 'b' fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured array is of shape (5,) and have values:\n",
      "[(661.94476  ,  945, 111.64480193) (662.6162   ,  573, 113.96664695)\n",
      " (972.33276  ,  966, 526.81905475) (  5.8173003,  713, 965.09302618)\n",
      " (467.66434  , 1015, 355.35753296)]\n",
      "\n",
      "This array converted to regular scalar matrix has shape: (15,) and values:\n",
      "[ 661.94476318  662.61621094  972.33276367    5.81730032  467.66433716\n",
      "  945.          573.          966.          713.         1015.\n",
      "  111.64480193  113.96664695  526.81905475  965.09302618  355.35753296]\n"
     ]
    }
   ],
   "source": [
    "num_elems = 5\n",
    "structured_array = np.zeros(shape=(num_elems), dtype=struct_dtype)\n",
    "structured_array['r'] = np.random.random(size=num_elems) * 1024\n",
    "structured_array['g'] = np.random.randint(0, high=1024, size=(num_elems))\n",
    "structured_array['b'] = np.random.random(size=num_elems) * 1024\n",
    "real_array = px.dtype_utils.flatten_compound_to_real(structured_array)\n",
    "\n",
    "print('Structured array is of shape {} and have values:'.format(structured_array.shape))\n",
    "print(structured_array)\n",
    "print('\\nThis array converted to regular scalar matrix has shape: {} and values:'.format(real_array.shape))\n",
    "print(real_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten_to_real()\n",
    "-----------------\n",
    "This function checks the data type of the provided dataset and then uses either of the above functions to (if necessary) flatten the dataset into a real-valued matrix. By checking the data type of the dataset, it obviates the need to explicitely call the aforementioend functions (that still do the work). Here is an example of the function being applied to the compound valued numpy array again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured array is of shape (5,) and have values:\n",
      "[(661.94476  ,  945, 111.64480193) (662.6162   ,  573, 113.96664695)\n",
      " (972.33276  ,  966, 526.81905475) (  5.8173003,  713, 965.09302618)\n",
      " (467.66434  , 1015, 355.35753296)]\n",
      "\n",
      "This array converted to regular scalar matrix has shape: (15,) and values:\n",
      "[ 661.94476318  662.61621094  972.33276367    5.81730032  467.66433716\n",
      "  945.          573.          966.          713.         1015.\n",
      "  111.64480193  113.96664695  526.81905475  965.09302618  355.35753296]\n"
     ]
    }
   ],
   "source": [
    "real_array = px.dtype_utils.flatten_to_real(structured_array)\n",
    "print('Structured array is of shape {} and have values:'.format(structured_array.shape))\n",
    "print(structured_array)\n",
    "print('\\nThis array converted to regular scalar matrix has shape: {} and values:'.format(real_array.shape))\n",
    "print(real_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three functions perform the inverse operation of taking real-valued matricies or datasets and converting them to complex or compound-valued datasets.\n",
    "\n",
    "stack_real_to_complex()\n",
    "-----------------------\n",
    "As the name suggests, this function collapses a N dimensional real-valued array of size (a, b, 2 * c) to a complex-valued array of shape (a, b, c). It assumes that the first c values in real-valued dataset are the real components and the following c values are the imaginary components of the complex value. This will become clearer with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real valued dataset of shape (12,):\n",
      "[2.53644478 3.515102   4.2621091  4.80083584 4.68737442 4.26197996\n",
      " 3.36456545 5.82320696 0.17763307 2.69691112 3.95820794 2.64769145]\n",
      "\n",
      "Complex-valued array of shape: (6,)\n",
      "[2.53644478+3.36456545j 3.515102  +5.82320696j 4.2621091 +0.17763307j\n",
      " 4.80083584+2.69691112j 4.68737442+3.95820794j 4.26197996+2.64769145j]\n"
     ]
    }
   ],
   "source": [
    "real_val = np.hstack([5 * np.random.rand(6), \n",
    "                      7 * np.random.rand(6)])\n",
    "print('Real valued dataset of shape {}:'.format(real_val.shape))\n",
    "print(real_val)\n",
    "\n",
    "comp_val = px.dtype_utils.stack_real_to_complex(real_val)\n",
    "\n",
    "print('\\nComplex-valued array of shape: {}'.format(comp_val.shape))\n",
    "print(comp_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stack_real_to_compound()\n",
    "-----------------------\n",
    "Similar to the above function, this function shrinks the last axis of a real valued dataset to create the desired compound valued dataset. Here we will demonstrate it on the same 3-field (r,g,b) compound datatype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real valued dataset of shape (15,):\n",
      "[770.45733449 659.01967622 119.29114206 917.70087711  62.73017631\n",
      " 399.         414.         306.          99.         366.\n",
      " 622.83035515 330.51481612 315.78275676 255.95941454  45.32541551]\n",
      "\n",
      "Structured array of shape: (5,)\n",
      "[(770.45734 , 399, 622.83035515) (659.01965 , 414, 330.51481612)\n",
      " (119.291145, 306, 315.78275676) (917.70087 ,  99, 255.95941454)\n",
      " ( 62.730175, 366,  45.32541551)]\n"
     ]
    }
   ],
   "source": [
    "num_elems = 5\n",
    "real_val = np.concatenate((np.random.random(size=num_elems) * 1024, \n",
    "                           np.random.randint(0, high=1024, size=num_elems), \n",
    "                           np.random.random(size=num_elems) * 1024))\n",
    "print('Real valued dataset of shape {}:'.format(real_val.shape))\n",
    "print(real_val)\n",
    "\n",
    "comp_val = px.dtype_utils.stack_real_to_compound(real_val, struct_dtype)\n",
    "\n",
    "print('\\nStructured array of shape: {}'.format(comp_val.shape))\n",
    "print(comp_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stack_real_to_target_dtype()\n",
    "-----------------------------\n",
    "This function performs the inverse of flatten_to_real() - stacks the provided real-valued dataet into a complex or compound valued dataset using the two above functions. Note that unlike flatten_to_real(), the target data type must be supplied to the function for this to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real valued dataset of shape (15,):\n",
      "[770.45733449 659.01967622 119.29114206 917.70087711  62.73017631\n",
      " 399.         414.         306.          99.         366.\n",
      " 622.83035515 330.51481612 315.78275676 255.95941454  45.32541551]\n",
      "\n",
      "Structured array of shape: (5,)\n",
      "[(770.45734 , 399, 622.83035515) (659.01965 , 414, 330.51481612)\n",
      " (119.291145, 306, 315.78275676) (917.70087 ,  99, 255.95941454)\n",
      " ( 62.730175, 366,  45.32541551)]\n"
     ]
    }
   ],
   "source": [
    "print('Real valued dataset of shape {}:'.format(real_val.shape))\n",
    "print(real_val)\n",
    "\n",
    "comp_val = px.dtype_utils.stack_real_to_target_dtype(real_val, struct_dtype)\n",
    "\n",
    "print('\\nStructured array of shape: {}'.format(comp_val.shape))\n",
    "print(comp_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check_dtype()\n",
    "--------------\n",
    "check_dtype() is a master function that figures out the data type, necessary function to transform a HDF5 dataset to a real-valued array, expected data shape, etc. Before we demonstrate this function, we need to quickly create an example HDF5 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = 'dtype_utils_example.h5'\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "with h5py.File(file_path) as h5_f:\n",
    "    num_elems = (5, 7)\n",
    "    structured_array = np.zeros(shape=num_elems, dtype=struct_dtype)\n",
    "    structured_array['r'] = 450 * np.random.random(size=num_elems)\n",
    "    structured_array['g'] = np.random.randint(0, high=1024, size=num_elems)\n",
    "    structured_array['b'] = 3178 * np.random.random(size=num_elems)\n",
    "    _ = h5_f.create_dataset('compound', data=structured_array)\n",
    "    _ = h5_f.create_dataset('real', data=450 * np.random.random(size=num_elems), dtype=np.float16)\n",
    "    _ = h5_f.create_dataset('complex', data=np.random.random(size=num_elems) +\n",
    "                                            1j * np.random.random(size=num_elems), dtype=np.complex64)\n",
    "    h5_f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets test the the function on compound-, complex-, and real-valued HDF5 datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking a compound-valued dataset:\n",
      "\tDataset being tested: <HDF5 dataset \"compound\": shape (5, 7), type \"|V14\">\n",
      "\tFunction to transform to real: <function flatten_compound_to_real at 0x108716d08>\n",
      "\tis_complex? False\n",
      "\tis_compound? True\n",
      "\tShape of dataset in its current form: (5, 7)\n",
      "\tAfter flattening to real, shape is expected to be: (5, 21)\n",
      "\tByte-size of a single element in its current form: 12\n",
      "\n",
      "Checking a complex-valued dataset:\n",
      "\tDataset being tested: <HDF5 dataset \"complex\": shape (5, 7), type \"<c8\">\n",
      "\tFunction to transform to real: <function flatten_complex_to_real at 0x108716c80>\n",
      "\tis_complex? True\n",
      "\tis_compound? False\n",
      "\tShape of dataset in its current form: (5, 7)\n",
      "\tAfter flattening to real, shape is expected to be: (5, 14)\n",
      "\tByte-size of a single element in its current form: 8\n",
      "\n",
      "Checking a real-valued dataset:\n",
      "\tDataset being tested: <HDF5 dataset \"real\": shape (5, 7), type \"<f2\">\n",
      "\tFunction to transform to real: <class 'numpy.float32'>\n",
      "\tis_complex? False\n",
      "\tis_compound? False\n",
      "\tShape of dataset in its current form: (5, 7)\n",
      "\tAfter flattening to real, shape is expected to be: (5, 7)\n",
      "\tByte-size of a single element in its current form: 4\n"
     ]
    }
   ],
   "source": [
    "def check_dataset(h5_dset):\n",
    "    print('\\tDataset being tested: {}'.format(h5_dset))\n",
    "    func, is_complex, is_compound, n_features, n_samples, type_mult = px.dtype_utils.check_dtype(h5_dset)\n",
    "    print('\\tFunction to transform to real: %s' % func)\n",
    "    print('\\tis_complex? %s' % is_complex)\n",
    "    print('\\tis_compound? %s' % is_compound)\n",
    "    print('\\tShape of dataset in its current form: {}'.format(h5_dset.shape))\n",
    "    print('\\tAfter flattening to real, shape is expected to be: ({}, {})'.format(n_samples, n_features))\n",
    "    print('\\tByte-size of a single element in its current form: {}'.format(type_mult))\n",
    "\n",
    "with h5py.File(file_path, mode='r') as h5_f:\n",
    "    print('Checking a compound-valued dataset:')\n",
    "    check_dataset(h5_f['compound'])\n",
    "    print('')\n",
    "    print('Checking a complex-valued dataset:')\n",
    "    check_dataset(h5_f['complex'])\n",
    "    print('')\n",
    "    print('Checking a real-valued dataset:')\n",
    "    check_dataset(h5_f['real'])\n",
    "os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
